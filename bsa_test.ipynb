{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import datetime\n",
    "import pytz\n",
    "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(model_path):\n",
    "  bert_model=AutoModel.from_pretrained(model_path)\n",
    "  return bert_model\n",
    "\n",
    "def make_tokenizer(model_path):\n",
    "  bert_tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "  return bert_tokenizer\n",
    "\n",
    "def split_data(data):\n",
    "  df_train ,df_test = train_test_split(data, test_size=0.15, random_state=101)\n",
    "  return df_train ,df_test\n",
    "\n",
    "\n",
    "def tokenize_data(data_df,tokenizer,label_dict):\n",
    "  SENT_COL='Data'\n",
    "  LAB_COL='Sentiment'\n",
    "  sents=list(data_df[SENT_COL].values)\n",
    "  labels=list(data_df[LAB_COL].values)\n",
    "  tokenized=tokenizer(sents,truncation=True,padding=True,max_length=256)\n",
    "  tokenized['input_ids']=torch.tensor([t for t in tokenized['input_ids']],dtype=torch.long,device=DEVICE)\n",
    "  tokenized['attention_mask']=torch.tensor([t for t in tokenized['attention_mask']],dtype=torch.long,device=DEVICE)\n",
    "  labels_num=[label_dict[lb] for lb in labels]    #convert the labels to numerical values, then convert that list to tensor\n",
    "  tokenized['labels']=torch.tensor([lb for lb in labels_num],dtype=torch.long,device=DEVICE)\n",
    "  return tokenized\n",
    "\n",
    "def make_data_loader(processed_data,train):\n",
    "  ts_data=TensorDataset(processed_data['input_ids'],processed_data['attention_mask'],processed_data['labels'])\n",
    "  if train:\n",
    "    return DataLoader(ts_data,shuffle=True,batch_size=40)\n",
    "  else:\n",
    "    return DataLoader(ts_data,shuffle=False,batch_size=16)\n",
    "\n",
    "def make_optimizer(model):\n",
    "  for param in model.bert_model.parameters():\n",
    "    param.requires_grad=False\n",
    "  list(model.bert_model.parameters())[-1].requires_grad=True\n",
    "  optimizer = torch.optim.Adam(\n",
    "      [\n",
    "      {\"params\":model.lin_layer.parameters(), \"lr\":3e-4},\n",
    "      {\"params\":model.bert_model.parameters(), \"lr\":2e-5}\n",
    "      ])\n",
    "  return optimizer\n",
    "\n",
    "\n",
    "def make_criterion():\n",
    "  criterion=nn.CrossEntropyLoss()\n",
    "  return criterion\n",
    "\n",
    "class MakeModel(nn.Module):\n",
    "  def __init__(self,model,k):\n",
    "    super(MakeModel,self).__init__()\n",
    "    self.bert_model=model\n",
    "    self.lin_layer=nn.Linear(768,k)\n",
    "    #nn.init.normal_(self.lin_layer.weight, std=0.02)\n",
    "    #nn.init.normal_(self.lin_layer.bias, 0)\n",
    "\n",
    "\n",
    "  def forward(self,input_ids,attention_mask):\n",
    "    out_vect=self.bert_model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "    lin_op=self.lin_layer(out_vect.last_hidden_state[:,0,:])\n",
    "    return F.softmax(lin_op)\n",
    "\n",
    "\n",
    "def train_epoch(train_loader,model,optimizer,loss_fn):\n",
    "  epoch_loss=0\n",
    "  for step,batch in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    batch=tuple(t.to(DEVICE) for t in batch)\n",
    "    input_ids,attention_mask,labels=batch\n",
    "    out_val=model(input_ids,attention_mask)\n",
    "    #print(out_val)\n",
    "    loss=loss_fn(out_val,labels)\n",
    "    epoch_loss+=loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  return epoch_loss/step\n",
    "\n",
    "def val_epoch(val_loader,model,loss_fn):\n",
    "  epoch_loss=0\n",
    "  with torch.no_grad():\n",
    "    for step,batch in enumerate(val_loader):\n",
    "      # optimizer.zero_grad()\n",
    "      batch=tuple(t.to(DEVICE) for t in batch)\n",
    "      input_ids,attention_mask,labels=batch\n",
    "      out_val=model(input_ids,attention_mask)\n",
    "      #print(out_val)\n",
    "      loss=loss_fn(out_val,labels)\n",
    "      epoch_loss+=loss.item()\n",
    "      # loss.backward()\n",
    "      # optimizer.step()\n",
    "    return epoch_loss/step\n",
    "\n",
    "def test(test_loader,model):\n",
    "    true_label=list()\n",
    "    pred_label=list()\n",
    "    with torch.no_grad():\n",
    "        for step,batch in tqdm(enumerate(test_loader)):\n",
    "            batch=tuple(t.to(DEVICE) for t in batch)\n",
    "            input_id,attention_mask,labels=batch\n",
    "            out_val=model(input_id,attention_mask)\n",
    "            out_labels=torch.argmax(out_val,-1)\n",
    "            true_label.extend(labels.detach().cpu().numpy().tolist())\n",
    "            pred_label.extend(out_labels.detach().cpu().numpy().tolist())\n",
    "        print(round(accuracy_score(true_label,pred_label),3))\n",
    "        target_names = ['Negative', 'Positive', 'Neutral']\n",
    "        print(classification_report(true_label, pred_label, target_names=target_names))\n",
    "\n",
    "\n",
    "def train_func(train_loader,val_loader,model):\n",
    "  loss_fn=make_criterion()\n",
    "  optimizer=make_optimizer(model)\n",
    "  EPOCHS=25\n",
    "  train_losses=list()\n",
    "  val_losses=list()\n",
    "  best_model, min_loss = None, 1000000\n",
    "  for i in tqdm(range(EPOCHS)):\n",
    "    train_loss=train_epoch(train_loader,model,optimizer,loss_fn)\n",
    "    val_loss=val_epoch(val_loader,model,loss_fn)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if val_loss < min_loss:\n",
    "      best_model = deepcopy(model)\n",
    "      min_loss = val_loss\n",
    "    print(f'epoch:{i}   train_loss: {round(train_loss,3)}  val_loss:{round(val_loss,3)}')\n",
    "  return best_model,train_losses,val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model ready for training\n",
    "model_path='sagorsarker/bangla-bert-base'        # set it with the pretrained bert model\n",
    "bert_model=make_model(model_path)\n",
    "bert_tokenizer=make_tokenizer(model_path)\n",
    "k=3     #fill it for number of labels in your data\n",
    "fin_model=MakeModel(bert_model,k)\n",
    "fin_model.to(DEVICE)\n",
    "\n",
    "# get the data ready\n",
    "df_train = pd.read_excel('/home/vikram/Antara/Bangla_Sentiment/train_v3.xlsx')\n",
    "train_data, val_data = split_data(df_train[:10000])\n",
    "uniq_lb = list(set(list(df_train['Sentiment'])))  #create the label_dict all the possible labels in the data\n",
    "label_dict = {lb:i for i,lb in enumerate(uniq_lb)}\n",
    "processed_train=tokenize_data(train_data,bert_tokenizer,label_dict)\n",
    "train_loader=make_data_loader(processed_train,True)\n",
    "processed_val=tokenize_data(val_data,bert_tokenizer,label_dict)\n",
    "val_loader=make_data_loader(processed_val,False)\n",
    "\n",
    "# train and save the model\n",
    "model_trained,train_losses,val_losses=train_func(train_loader,val_loader,fin_model)\n",
    "PATH = '/home/vikram/Antara/Bangla_Sentiment/bsenti_model'\n",
    "torch.save(model_trained, f'{PATH}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_765612/1056614818.py:61: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(lin_op)\n",
      "144it [00:29,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.50      0.48      0.49      2313\n",
      "    Positive       0.49      0.37      0.42      1592\n",
      "     Neutral       0.39      0.49      0.43      1833\n",
      "\n",
      "    accuracy                           0.45      5738\n",
      "   macro avg       0.46      0.45      0.45      5738\n",
      "weighted avg       0.46      0.45      0.45      5738\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path='sagorsarker/bangla-bert-base'\n",
    "bert_tokenizer=make_tokenizer(model_path)\n",
    "test_data = pd.read_excel('/home/vikram/Antara/Bangla_Sentiment/test_v3.xlsx')\n",
    "uniq_lb = list(set(list(test_data['Sentiment'])))\n",
    "label_dict = {lb:i for i,lb in enumerate(uniq_lb)}\n",
    "processed_test=tokenize_data(test_data,bert_tokenizer,label_dict)\n",
    "test_loader=make_data_loader(processed_test,True)\n",
    "PATH = '/home/vikram/Antara/Bangla_Sentiment/bsenti_model.pt'\n",
    "model = torch.load(PATH)\n",
    "model.to(DEVICE)\n",
    "test(test_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [00:19,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.54      0.43      0.48      1592\n",
      "    Positive       0.51      0.49      0.50      2313\n",
      "     Neutral       0.41      0.51      0.46      1833\n",
      "\n",
      "    accuracy                           0.48      5738\n",
      "   macro avg       0.49      0.48      0.48      5738\n",
      "weighted avg       0.49      0.48      0.48      5738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path='csebuetnlp/banglabert'\n",
    "bert_tokenizer=make_tokenizer(model_path)\n",
    "test_data = pd.read_excel('/home/vikram/Antara/Bangla_Sentiment/test_v3.xlsx')\n",
    "uniq_lb = list(set(list(test_data['Sentiment'])))\n",
    "label_dict = {lb:i for i,lb in enumerate(uniq_lb)}\n",
    "processed_test=tokenize_data(test_data,bert_tokenizer,label_dict)\n",
    "test_loader=make_data_loader(processed_test,True)\n",
    "PATH = '/home/vikram/Antara/Bangla_Sentiment/bsenti_model3.pt'\n",
    "model = torch.load(PATH)\n",
    "model.to(DEVICE)\n",
    "test(test_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data_inf(tokenizer,sent):\n",
    "    tokenized_path=tokenizer(sent,truncation=True,padding=True,max_length=256,return_tensors='pt').to(DEVICE)\n",
    "    return tokenized_path['input_ids'],tokenized_path['attention_mask']\n",
    "\n",
    "\n",
    "def inference_code(sent,cat,tokenizer,model):\n",
    "\tinput_ids,attn_mask=tokenize_data_inf(tokenizer,sent)\n",
    "\tout_vals=model(input_ids,attn_mask)\n",
    "\tlabl=torch.argmax(out_vals,1)\n",
    "\tout_list=out_vals.detach().cpu().numpy().tolist()\n",
    "\tlabel_dict = {0:'Negative', 1:'Positive', 2:'Neutral'}\n",
    "\toutput_dict=dict()\n",
    "\tfor i in range(len(out_list[0])):\n",
    "\t\toutput_dict[label_dict[i]]= round(out_list[0][i],3)\n",
    "\tcurrent_time = datetime.datetime.now(pytz.timezone('Asia/Kolkata'))\n",
    "\tdate = str(current_time.day) + '/' + str(current_time.month) + '/' + str(current_time.year)\n",
    "\ttime = str(current_time.hour) + ':' + str(current_time.minute) + ':' + str(current_time.second)\n",
    "\toutput_dict['Category'] = cat\n",
    "\toutput_dict['Date'] = date\n",
    "\toutput_dict['Time of generation'] = time\n",
    "\treturn output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Negative': 0.004, 'Positive': 0.979, 'Neutral': 0.017, 'Category': 'international', 'Date': '26/11/2023', 'Time of generation': '0:17:6'}\n"
     ]
    }
   ],
   "source": [
    "text = \"পছন্দসই মিশ্র পাঠে হাজিরায় ফাঁকি নয়: পার্থ\"\n",
    "cat = 'international'\n",
    "model_path='csebuetnlp/banglabert'\n",
    "bert_tokenizer=make_tokenizer(model_path)\n",
    "PATH = '/home/vikram/Antara/Bangla_Sentiment/bsenti_model3.pt'\n",
    "model = torch.load(PATH)\n",
    "model.to(DEVICE)\n",
    "res_dict = inference_code(text,cat,bert_tokenizer,model)\n",
    "print(res_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
